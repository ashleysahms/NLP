{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VietnameseNewDataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75uXRDhktilr"
      },
      "source": [
        "\n",
        "We are using Google Colab due to the GPUs and the quick runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9z1JIA7HIVt"
      },
      "source": [
        "Code is largely based off of the following source: https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfbTk5gZuK9V",
        "outputId": "bb34f44d-e8c9-475a-ec54-2f59ef0d297c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWVZkxBMuMYn"
      },
      "source": [
        "import pandas as pd\n",
        "path = '/content/drive/MyDrive/vie.txt'\n",
        "df = pd.read_csv(path, header= None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sc6B8ckxnZz"
      },
      "source": [
        "df=df.drop(columns=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "n4FwpXkCYK2c",
        "outputId": "a87ae56c-061b-4856-8e0c-c337f0ad45c5"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Chạy!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Help!</td>\n",
              "      <td>Giúp tôi với!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go on.</td>\n",
              "      <td>Tiếp tục đi.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>Chào bạn.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hurry!</td>\n",
              "      <td>Nhanh lên nào!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6364</th>\n",
              "      <td>In 2009, Selena Gomez became the youngest pers...</td>\n",
              "      <td>Vào năm 2009, Sê-lê-na Gô-mét đã được lựa chọn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6365</th>\n",
              "      <td>In 2009, Selena Gomez became the youngest pers...</td>\n",
              "      <td>Vào năm 2009, Selena Gomez đã được lựa chọn để...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6366</th>\n",
              "      <td>In 2009, Selena Gomez became the youngest pers...</td>\n",
              "      <td>Vào năm 2009, Selena Gomez đã trở thành Đại sứ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6367</th>\n",
              "      <td>The people here are particular about what they...</td>\n",
              "      <td>Những người ở đây khá là khó tính về khẩu vị ă...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6368</th>\n",
              "      <td>No matter how much you try to convince people ...</td>\n",
              "      <td>Cho dù bạn có thuyết phục mọi người rằng sô-cô...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6369 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0                                                  1\n",
              "0                                                  Run!                                              Chạy!\n",
              "1                                                 Help!                                      Giúp tôi với!\n",
              "2                                                Go on.                                       Tiếp tục đi.\n",
              "3                                                Hello!                                          Chào bạn.\n",
              "4                                                Hurry!                                     Nhanh lên nào!\n",
              "...                                                 ...                                                ...\n",
              "6364  In 2009, Selena Gomez became the youngest pers...  Vào năm 2009, Sê-lê-na Gô-mét đã được lựa chọn...\n",
              "6365  In 2009, Selena Gomez became the youngest pers...  Vào năm 2009, Selena Gomez đã được lựa chọn để...\n",
              "6366  In 2009, Selena Gomez became the youngest pers...  Vào năm 2009, Selena Gomez đã trở thành Đại sứ...\n",
              "6367  The people here are particular about what they...  Những người ở đây khá là khó tính về khẩu vị ă...\n",
              "6368  No matter how much you try to convince people ...  Cho dù bạn có thuyết phục mọi người rằng sô-cô...\n",
              "\n",
              "[6369 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vs7QcLWZDan"
      },
      "source": [
        "This dataset has many duplicates (English -> Vietnamese). While it might be beneficial for a person studying a language to have these variations of phrases, due to the simplicity of our model, we chose to remove these duplicates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "cfJShUEeZy-7",
        "outputId": "df6a5eda-403b-4235-a07b-737e7aea5cac"
      },
      "source": [
        "df_simple = df.drop_duplicates([0])\n",
        "df_simple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Chạy!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Help!</td>\n",
              "      <td>Giúp tôi với!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go on.</td>\n",
              "      <td>Tiếp tục đi.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>Chào bạn.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hurry!</td>\n",
              "      <td>Nhanh lên nào!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6361</th>\n",
              "      <td>The people crowded round the injured man, but ...</td>\n",
              "      <td>Nhiều người đã vây xung quanh người bị thương,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6362</th>\n",
              "      <td>It's very easy to sound natural in your own na...</td>\n",
              "      <td>Câu nói đó trong ngôn ngữ của bạn rất dễ để ng...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6363</th>\n",
              "      <td>In 2009, Selena Gomez became the youngest pers...</td>\n",
              "      <td>Vào năm 2009, Selena Gomez đã được lựa chọn để...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6367</th>\n",
              "      <td>The people here are particular about what they...</td>\n",
              "      <td>Những người ở đây khá là khó tính về khẩu vị ă...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6368</th>\n",
              "      <td>No matter how much you try to convince people ...</td>\n",
              "      <td>Cho dù bạn có thuyết phục mọi người rằng sô-cô...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5074 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0                                                  1\n",
              "0                                                  Run!                                              Chạy!\n",
              "1                                                 Help!                                      Giúp tôi với!\n",
              "2                                                Go on.                                       Tiếp tục đi.\n",
              "3                                                Hello!                                          Chào bạn.\n",
              "4                                                Hurry!                                     Nhanh lên nào!\n",
              "...                                                 ...                                                ...\n",
              "6361  The people crowded round the injured man, but ...  Nhiều người đã vây xung quanh người bị thương,...\n",
              "6362  It's very easy to sound natural in your own na...  Câu nói đó trong ngôn ngữ của bạn rất dễ để ng...\n",
              "6363  In 2009, Selena Gomez became the youngest pers...  Vào năm 2009, Selena Gomez đã được lựa chọn để...\n",
              "6367  The people here are particular about what they...  Những người ở đây khá là khó tính về khẩu vị ă...\n",
              "6368  No matter how much you try to convince people ...  Cho dù bạn có thuyết phục mọi người rằng sô-cô...\n",
              "\n",
              "[5074 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J26-VimlyNB3"
      },
      "source": [
        "df_simple.to_csv(r'/content/drive/MyDrive/vie_simple.txt', header=None, index=None, sep='\\t', mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p84U4WESZS45"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJrCepQf13Qx",
        "outputId": "5842b5b5-73a2-4aea-c76c-f859c4cc371c"
      },
      "source": [
        "# load dataset\n",
        "filename = '/content/drive/MyDrive/vie_simple.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-vietnamese pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-vietnamese.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-vietnamese.pkl\n",
            "[run] => [chay]\n",
            "[help] => [giup toi voi]\n",
            "[go on] => [tiep tuc i]\n",
            "[hello] => [chao ban]\n",
            "[hurry] => [nhanh len nao]\n",
            "[eat it] => [an i]\n",
            "[help me] => [cuu toi voi]\n",
            "[i agree] => [toi cung nghi nhu vay]\n",
            "[perfect] => [hoan hao]\n",
            "[we know] => [chung toi biet]\n",
            "[you run] => [ban chay]\n",
            "[cheer up] => [ung co rau ri qua nhu the]\n",
            "[he tries] => [han thu]\n",
            "[hurry up] => [thoang cai chan len]\n",
            "[i forgot] => [toi quen mat roi]\n",
            "[im bald] => [toi bi hoi]\n",
            "[im busy] => [toi ang ban]\n",
            "[too late] => [muon qua]\n",
            "[i hate tv] => [toi ghet ti vi]\n",
            "[i laughed] => [toi a cuoi]\n",
            "[i will go] => [toi se i]\n",
            "[is it bad] => [no co toi khong]\n",
            "[its cold] => [lanh]\n",
            "[its ours] => [o la cua chung toi]\n",
            "[she cried] => [no a khoc]\n",
            "[sit there] => [hay ngoi o o]\n",
            "[whats up] => [gi the]\n",
            "[are you ok] => [ban co sao khong]\n",
            "[find a job] => [hay tim mot cong viec i]\n",
            "[hurry home] => [hay mau ve nha i]\n",
            "[i can read] => [toi oc uoc]\n",
            "[i guess so] => [toi oan vay]\n",
            "[i hate you] => [toi ghet anh]\n",
            "[i love you] => [anh phai long em]\n",
            "[i will try] => [toi se thu]\n",
            "[im lonely] => [toi cam thay co on]\n",
            "[im so fat] => [toi beo qua]\n",
            "[its magic] => [o la phep thuat]\n",
            "[let me try] => [e toi thu]\n",
            "[no kidding] => [khong ua chu]\n",
            "[time is up] => [thoi gian a het]\n",
            "[tom winked] => [tom a nhay mat]\n",
            "[whos here] => [ai ay]\n",
            "[be punctual] => [hay ung gio]\n",
            "[dont fight] => [ung anh nhau]\n",
            "[i heard you] => [toi nghe ban roi]\n",
            "[i like both] => [toi thich ca hai cai]\n",
            "[i like jazz] => [toi thich nhac jazz]\n",
            "[i like math] => [toi thich toan]\n",
            "[i live here] => [toi song o ay]\n",
            "[i need more] => [toi can them]\n",
            "[i saw a ufo] => [toi nhin thay ufo]\n",
            "[i saw a dog] => [toi thay mot con cho]\n",
            "[im nervous] => [toi lo lang]\n",
            "[im not tom] => [toi khong phai tom]\n",
            "[im smashed] => [toi a say ruou]\n",
            "[im thirsty] => [toi khat nuoc]\n",
            "[it suits me] => [no vua voi toi]\n",
            "[lets do it] => [thuc hien thoi]\n",
            "[may i begin] => [toi co the bat au uoc khong]\n",
            "[nobody came] => [khong ai toi het]\n",
            "[she is kind] => [co ay la mot nguoi tot bung]\n",
            "[tom saw you] => [tom a trong thay ban]\n",
            "[vote for me] => [hay bau cho toi]\n",
            "[we miss tom] => [chung toi nho tom]\n",
            "[whats that] => [cai nay la cai gi]\n",
            "[whats this] => [ay la cai gi]\n",
            "[youre mine] => [em la cua anh]\n",
            "[anybody home] => [co ai o nha khong]\n",
            "[are you cold] => [ban co lanh khong]\n",
            "[are you okay] => [ban co sao khong]\n",
            "[come help me] => [hay en cuu toi]\n",
            "[come with me] => [i voi toi]\n",
            "[dont be shy] => [ung ngai]\n",
            "[drive faster] => [lai nhanh hon i]\n",
            "[hang on tom] => [cho chut tom]\n",
            "[i am at home] => [toi ang o nha]\n",
            "[i am curious] => [toi to mo]\n",
            "[i dont know] => [toi khong biet]\n",
            "[i hear music] => [toi nghe nhac]\n",
            "[i like bread] => [toi thich banh mi]\n",
            "[i wont lose] => [toi khong thua au]\n",
            "[id buy that] => [lay cho toi cai kia]\n",
            "[im not free] => [toi khong ranh]\n",
            "[im retiring] => [toi sap nghi huu]\n",
            "[is tom there] => [tom co o o khong]\n",
            "[its raining] => [troi ang mua]\n",
            "[its so easy] => [qua de]\n",
            "[its so hard] => [viec nay that la kho]\n",
            "[its the law] => [o la luat]\n",
            "[keep looking] => [cu tiep tuc nhin i]\n",
            "[let tom stay] => [e tom o lai]\n",
            "[let tom wait] => [hay e tom oi]\n",
            "[let me dream] => [hay e toi mo mong]\n",
            "[lets try it] => [thu no xem sao]\n",
            "[my eyes hurt] => [mat to au]\n",
            "[now lets go] => [gio chung ta i thoi nao]\n",
            "[she liked it] => [co ay thich no]\n",
            "[stop filming] => [dung ghi hinh]\n",
            "[that is mine] => [cai o la cua toi]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI01DytRdX7g",
        "outputId": "17fb1342-a82f-493c-e08d-9962799b4a4c"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-vietnamese.pkl')\n",
        "# reduce dataset size\n",
        "n_sentences = 1000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:900], dataset[900:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-vietnamese-both.pkl')\n",
        "save_clean_data(train, 'english-vietnamese-train.pkl')\n",
        "save_clean_data(test, 'english-vietnamese-test.pkl')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-vietnamese-both.pkl\n",
            "Saved: english-vietnamese-train.pkl\n",
            "Saved: english-vietnamese-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mciMUUGmf2Me"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTxX2Dije72E"
      },
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-vietnamese-both.pkl')\n",
        "train = load_clean_sentences('english-vietnamese-train.pkl')\n",
        "test = load_clean_sentences('english-vietnamese-test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gvH_1K9e8D1"
      },
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "  \n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lEPx16Ze8Rp",
        "outputId": "6422603c-8cf6-4f4d-875f-fa6fefae61ae"
      },
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "\n",
        "# prepare vietnamese tokenizer\n",
        "vie_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "vie_vocab_size = len(vie_tokenizer.word_index) + 1\n",
        "vie_length = max_length(dataset[:, 1])\n",
        "print('Vietnamese Vocabulary Size: %d' % vie_vocab_size)\n",
        "print('Vietnamese Max Length: %d' % (vie_length))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 916\n",
            "English Max Length: 6\n",
            "Vietnamese Vocabulary Size: 552\n",
            "Vietnamese Max Length: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wdMhHq2t0qq"
      },
      "source": [
        "We added one hot encoding to create a binary for our categorical variables. Doing this will allow the model to run properly as categorical variables cannot be used in most machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rssH-wyfSxg"
      },
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRxvgu1YfS0F"
      },
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(vie_tokenizer, vie_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(vie_tokenizer, vie_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-c7HSAzt4kq"
      },
      "source": [
        "We chose to utilize the Sequential Sequence2Sequence model from keras in order to give an output to a sequence of words given a sequence of words. Seq2Seq runs based on encoders and decoders and can automatically learn mapped sentence pairs in each language in order to predict an output sentence. We chose this model because it works best with our data and completes the job we need - translating one language to another.\n",
        "\n",
        "Bidirectional LSTM was used to gain a better performance of our model. Bidirectional LSTM trains on two LSTM layers rather than one, giving the model better results overall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDAVBCJtfS3k",
        "outputId": "bba3fbb4-129f-4a7a-f4bf-c8b6221dcb27"
      },
      "source": [
        "from keras.layers.wrappers import Bidirectional\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(Bidirectional(LSTM(n_units)))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(Bidirectional(LSTM(n_units, return_sequences=True)))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# define model\n",
        "model = define_model(vie_vocab_size, eng_vocab_size, vie_length, eng_length, 256)\n",
        "#rms = optimizers.RMSprop(lr=0.0001)\n",
        "model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#optimizer=rms\n",
        "\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "#plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 11, 256)           141312    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 512)               1050624   \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 6, 512)            0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 6, 512)            1574912   \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 6, 916)            469908    \n",
            "=================================================================\n",
            "Total params: 3,236,756\n",
            "Trainable params: 3,236,756\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emCK88cPfS6Z",
        "outputId": "7aad812f-65cb-489f-8115-2dc6412afd62"
      },
      "source": [
        "# fit model\n",
        "filename = 'model.h6'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "15/15 - 42s - loss: 6.0459 - accuracy: 0.3372 - val_loss: 4.3907 - val_accuracy: 0.3700\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.39072, saving model to model.h6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/30\n",
            "15/15 - 0s - loss: 4.1057 - accuracy: 0.3917 - val_loss: 4.0745 - val_accuracy: 0.4100\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.39072 to 4.07452, saving model to model.h6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3/30\n",
            "15/15 - 0s - loss: 3.7878 - accuracy: 0.3957 - val_loss: 3.9547 - val_accuracy: 0.4100\n",
            "\n",
            "Epoch 00003: val_loss improved from 4.07452 to 3.95474, saving model to model.h6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/30\n",
            "15/15 - 0s - loss: 3.6809 - accuracy: 0.3957 - val_loss: 3.9355 - val_accuracy: 0.4100\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.95474 to 3.93554, saving model to model.h6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5/30\n",
            "15/15 - 0s - loss: 3.6287 - accuracy: 0.4004 - val_loss: 3.9395 - val_accuracy: 0.4117\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 3.93554\n",
            "Epoch 6/30\n",
            "15/15 - 0s - loss: 3.5896 - accuracy: 0.3978 - val_loss: 3.9127 - val_accuracy: 0.4183\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.93554 to 3.91267, saving model to model.h6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7/30\n",
            "15/15 - 0s - loss: 3.5500 - accuracy: 0.4063 - val_loss: 3.9212 - val_accuracy: 0.4200\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 3.91267\n",
            "Epoch 8/30\n",
            "15/15 - 0s - loss: 3.5180 - accuracy: 0.4061 - val_loss: 3.9292 - val_accuracy: 0.4183\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 3.91267\n",
            "Epoch 9/30\n",
            "15/15 - 0s - loss: 3.4758 - accuracy: 0.4198 - val_loss: 3.9256 - val_accuracy: 0.4233\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 3.91267\n",
            "Epoch 10/30\n",
            "15/15 - 0s - loss: 3.4272 - accuracy: 0.4261 - val_loss: 3.8915 - val_accuracy: 0.4500\n",
            "\n",
            "Epoch 00010: val_loss improved from 3.91267 to 3.89151, saving model to model.h6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/30\n",
            "15/15 - 0s - loss: 3.3531 - accuracy: 0.4374 - val_loss: 3.8802 - val_accuracy: 0.4467\n",
            "\n",
            "Epoch 00011: val_loss improved from 3.89151 to 3.88021, saving model to model.h6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12/30\n",
            "15/15 - 0s - loss: 3.2704 - accuracy: 0.4452 - val_loss: 3.8958 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 3.88021\n",
            "Epoch 13/30\n",
            "15/15 - 0s - loss: 3.1661 - accuracy: 0.4524 - val_loss: 3.8676 - val_accuracy: 0.4550\n",
            "\n",
            "Epoch 00013: val_loss improved from 3.88021 to 3.86761, saving model to model.h6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.h6/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14/30\n",
            "15/15 - 0s - loss: 3.0737 - accuracy: 0.4602 - val_loss: 3.8781 - val_accuracy: 0.4583\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 3.86761\n",
            "Epoch 15/30\n",
            "15/15 - 0s - loss: 2.9934 - accuracy: 0.4689 - val_loss: 3.9755 - val_accuracy: 0.4583\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 3.86761\n",
            "Epoch 16/30\n",
            "15/15 - 0s - loss: 2.8926 - accuracy: 0.4767 - val_loss: 3.9236 - val_accuracy: 0.4683\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 3.86761\n",
            "Epoch 17/30\n",
            "15/15 - 0s - loss: 2.7985 - accuracy: 0.4817 - val_loss: 3.9756 - val_accuracy: 0.4633\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 3.86761\n",
            "Epoch 18/30\n",
            "15/15 - 0s - loss: 2.6988 - accuracy: 0.4831 - val_loss: 4.0073 - val_accuracy: 0.4467\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 3.86761\n",
            "Epoch 19/30\n",
            "15/15 - 0s - loss: 2.6055 - accuracy: 0.4881 - val_loss: 4.0232 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 3.86761\n",
            "Epoch 20/30\n",
            "15/15 - 0s - loss: 2.4863 - accuracy: 0.4989 - val_loss: 4.0892 - val_accuracy: 0.4617\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 3.86761\n",
            "Epoch 21/30\n",
            "15/15 - 0s - loss: 2.4010 - accuracy: 0.5085 - val_loss: 4.1487 - val_accuracy: 0.4683\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 3.86761\n",
            "Epoch 22/30\n",
            "15/15 - 0s - loss: 2.4136 - accuracy: 0.5015 - val_loss: 4.1956 - val_accuracy: 0.4583\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 3.86761\n",
            "Epoch 23/30\n",
            "15/15 - 0s - loss: 2.2975 - accuracy: 0.5146 - val_loss: 4.1388 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 3.86761\n",
            "Epoch 24/30\n",
            "15/15 - 0s - loss: 2.1772 - accuracy: 0.5183 - val_loss: 4.2149 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 3.86761\n",
            "Epoch 25/30\n",
            "15/15 - 0s - loss: 2.0548 - accuracy: 0.5365 - val_loss: 4.1798 - val_accuracy: 0.4600\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 3.86761\n",
            "Epoch 26/30\n",
            "15/15 - 0s - loss: 1.9639 - accuracy: 0.5465 - val_loss: 4.2413 - val_accuracy: 0.4533\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 3.86761\n",
            "Epoch 27/30\n",
            "15/15 - 0s - loss: 1.8408 - accuracy: 0.5663 - val_loss: 4.2356 - val_accuracy: 0.4583\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 3.86761\n",
            "Epoch 28/30\n",
            "15/15 - 0s - loss: 1.7645 - accuracy: 0.5820 - val_loss: 4.2688 - val_accuracy: 0.4383\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 3.86761\n",
            "Epoch 29/30\n",
            "15/15 - 0s - loss: 1.6822 - accuracy: 0.5937 - val_loss: 4.2654 - val_accuracy: 0.4450\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 3.86761\n",
            "Epoch 30/30\n",
            "15/15 - 0s - loss: 1.5753 - accuracy: 0.6217 - val_loss: 4.2861 - val_accuracy: 0.4450\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 3.86761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f932a0cd690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkNQ9KUJjCRl"
      },
      "source": [
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h6')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNX_K3fojCtU"
      },
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src = dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL8UM4EqHZ5-",
        "outputId": "2ec2aa25-c047-4553-8388-ab62c9d223bf"
      },
      "source": [
        "#Attempt 3 of 5/8/21 (bidirectional lstm layers)\n",
        "\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[toi thich anh nen], target=[i like candlelight], predicted=[i i]\n",
            "src=[khong co gioi han], target=[there are no limits], predicted=[is is]\n",
            "src=[to tung bi bat coc], target=[i was kidnapped], predicted=[i i]\n",
            "src=[ban co the lam uoc], target=[you can do it], predicted=[you you you]\n",
            "src=[toi can co mot may vi tinh moi], target=[i need a new computer], predicted=[i i to to]\n",
            "src=[tai sao cau lai hon tom], target=[why did you kiss tom], predicted=[tom tom tom a]\n",
            "src=[tiep tuc i], target=[go on], predicted=[is is]\n",
            "src=[ang ua ay a], target=[youre joking], predicted=[is is]\n",
            "src=[ai a lam chiec banh nay], target=[who made this cake], predicted=[is is]\n",
            "src=[bon minh can nghi lon hon], target=[we need to think big], predicted=[i i a a]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.169236\n",
            "BLEU-2: 0.020689\n",
            "BLEU-3: 0.087374\n",
            "BLEU-4: 0.125255\n",
            "test\n",
            "src=[ho a i cau ca], target=[they went fishing], predicted=[you is a]\n",
            "src=[chung ta khong the bo cuoc], target=[we cant give up], predicted=[its is a]\n",
            "src=[ho bat toi], target=[they arrested me], predicted=[i to]\n",
            "src=[rat hiem loi phan nan], target=[complaints are rare], predicted=[its is]\n",
            "src=[ban co the vao], target=[you may enter], predicted=[you you you]\n",
            "src=[thu nay lam viec tot qua], target=[this is working great], predicted=[is is a]\n",
            "src=[hai nguoi o lai ay], target=[you two stay here], predicted=[is is]\n",
            "src=[khong co gioi han], target=[there is no limit], predicted=[is is]\n",
            "src=[tat ca moi nguoi eu ghet tom], target=[everybody hates tom], predicted=[tom tom a]\n",
            "src=[toi bi bat a], target=[am i under arrest], predicted=[i i]\n",
            "BLEU-1: 0.142855\n",
            "BLEU-2: 0.023481\n",
            "BLEU-3: 0.093598\n",
            "BLEU-4: 0.132252\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}